<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Tue Aug 10 2021 20:07:52 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="60fdcbff5dbbe9de78a22a58" data-wf-site="607dcf321ef9c11104c38e66">
<head>
  <meta charset="utf-8">
  <title>Prototyping | The Future of Digital Television</title>
  <meta content="Prototyping | The Future of Digital Television" property="og:title">
  <meta content="Prototyping | The Future of Digital Television" property="twitter:title">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/interdigital.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inter:100,200,300,regular,500,600,700,800,900","Space Grotesk:300,regular,500,600,700"]  }});</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tween.js/16.3.5/Tween.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.6.1/gsap.min.js" integrity="sha512-cdV6j5t5o24hkSciVrb8Ki6FveC2SgwGfLE31+ZQRHAeSRxYhAQskLkq3dLm8ZcWe1N3vBOEYmmbhzf7NTtFFQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.7.1/ScrollTrigger.min.js" integrity="sha512-DlTk2PLUinhBupE89kHOJTt11QqbRMQVlbb26XVDvp4D1kt0fRvQJslvZnTelRJHq6yK0tIPCR7cul8+9Blz0g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.7.1/ScrollToPlugin.min.js" integrity="sha512-1OG9UO4krPizjtz/c9iDbjCqtXznBYdJeD4ccPaYfJHzC6F1qoQ3P1bgQ3J8lgCoK5qGVCqsY4+/RKjLDzITVQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Inconsolata:400,700","Inter:100,200,300,regular,500,600,700,800,900","Space Grotesk:300,regular,500,600,700"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body-2" data-index="3">
  <video id="prototypeTV" autoplay muted loop crossOrigin="anonymous" style="display:none">
    <source src="./Textures/PrototypingVideoTextureFlip.mp4" type='video/mp4'>
  </video>
  <div data-w-id="543a9394-3540-05ad-8a51-15890bdcee16" class="title-section process-title-section wf-section">
    <h1 class="title-card">process<br><strong class="bold-text-11">Design Considerations</strong></h1>
  </div>
  <div class="scroll-ind-section wf-section">
    <div class="scroll-ind-div">
      <div data-w-id="cfcb7a03-7b10-b9fa-fed6-b7937f7e9f6d" data-animation-type="lottie" data-src="documents/lf30_editor_rdgpmu2e.json" data-loop="1" data-direction="1" data-autoplay="1" data-is-ix2-target="0" data-renderer="svg" data-default-duration="3" data-duration="0" class="scroll_lottie"></div>
    </div>
  </div>
  <div class="content-section toc-section wf-section">
    <div class="div-block-7">
      <h3 class="paragraph-heading heading-space-long">Process</h3>
      <div class="w-layout-grid grid">
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">01</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text">Overview</div>
          </div>
        </div>
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">02</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text">Pretotype</div>
          </div>
        </div>
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">03</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text">Affinity Diagram</div>
          </div>
        </div>
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">04</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text toc-text-tl">Contextual Inquiry</div>
          </div>
        </div>
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">05</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text">Diary Study</div>
          </div>
        </div>
        <div class="toc-column w-row">
          <div class="w-col w-col-3">
            <div class="toc-index">06</div>
          </div>
          <div class="w-col w-col-9">
            <div class="toc-text toc-text-tl">Conceptual Prototype</div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="goto-pretotype" class="content-section wf-section">
    <div id="navigation-anchor" class="side-nav-div">
      <div class="div-block-21">
        <a href="#overview" class="sidenav-link">OVERVIEW</a>
        <a href="#emotional-context" class="sidenav-link">EMOTIONAL CONTEXT</a>
        <a href="#attention" class="sidenav-link">ATTENTION LEVEL</a>
        <a href="#historic-viewing" class="sidenav-link">HISTORIC VIEWING DATA</a>
        <a href="#user-control" class="sidenav-link">USER CONTROL</a>
        <a href="#risk-factors" class="sidenav-link">RISK FACTORS</a>
      </div>
      <div class="div-block-22">
        <a href="#overview" class="button w-button sidenav-link"></a>
        <a href="#emotional-context" class="button w-button sidenav-link"></a>
        <a href="#attention" class="button w-button sidenav-link"></a>
        <a href="#historic-viewing" class="button w-button sidenav-link"></a>
        <a href="#user-control" class="button w-button sidenav-link"></a>
        <a href="#risk-factors" class="button w-button sidenav-link"></a>
      </div>
    </div>
    <div class="div-block-30">
      <div data-w-id="03440689-c2cd-ca48-9f75-5c46ff05ca76" class="main-col-div">
        <div id="overview" data-w-id="fe383e66-a6db-63fe-4674-ec42df43d5a1" class="div-block-7">
          <p class="paragraph-content">Whether we’re listening to a podcast while driving, listening to music while cooking, or watching a show that will help us relax after a long day at work - Content consumption is integral to our routine, regardless of the amount of attention we have to give or the mood we’re in. We expect our content to not only entertain us, but to <strong>flow with us</strong> throughout our day, fit into our routine, and adapt to our emotions. If we’ve had a long day at work, we’ll watch a drama to help us get out of our heads. If we’re doing chores around the house, we’ll rewatch The Office for the fourth time so we can be entertained despite our wavering attention.<br><br>We began envisioning a future where TV content would synthetically adapt to user&#x27;s context, specifically their <a href="#"><strong class="bold-text-26">emotions</strong></a>, <a href="#"><strong class="bold-text-28">attention</strong></a>, and personal viewing <a href="#historic-viewing"><strong class="bold-text-27">preferences</strong></a>. </p>
          <h1 class="heading-2"><br>Considerations</h1>
          <h3 id="emotional-context" class="heading-3">emotional context</h3>
        </div>
        <p class="paragraph-content">It&#x27;s no secret that we use TV to feed our emotional needs. Whether we put on a tearjerker to wallow in our sadness, or counteract it with something happy, it became clear that users let their mood dictate the content they consume and anticipate that content would further regulate or change to adapt to their mood in the future.</p><img src="images/PrototypeImg_0.png" loading="lazy" sizes="(max-width: 479px) 74vw, (max-width: 767px) 250px, (max-width: 991px) 54vw, 58vw" srcset="images/PrototypeImg_0-p-500.png 500w, images/PrototypeImg_0.png 766w" alt="two people watching TV together on the couch. Person 1 says &quot;today is such a stressful work day. Let me take a break and watch an episode of the Great British Baking Show.&quot; Person 2 says &quot;I&#x27;m really upset about the fight I just had with Jess. I really need an emotional outlet. Let me find a K drama.&quot;" class="image">
        <p class="paragraph-content">Emotional responses and consequential content adaptations became a focal point of our prototyping for the future of digital television experiences. Users weren&#x27;t too keen on plot changes. TV is in a golden age, and most wanted to experience what the writers intended, but there were several other ways we could adapt and elicit this emotional reaction including changes to:</p><img src="images/Frame-2-3.png" loading="lazy" alt="Color Schemes, Background Music, and Environmental Features">
        <p class="paragraph-content">Directing the plot on particular branch paths can be a dramatic adaptation for users during a content consumption experience and while stylistic adaptations are slightly more subtle, they can make just as much of an impact on a user’s experience by conforming to a user’s current and desired emotional state. Plot changes have more value on lower effort content or rewatches, a supplement to watching with the original plot and then shifting to show the user something new or interesting.</p>
      </div>
      <div id="attention" data-w-id="59e34e61-efce-648d-4c80-c4a33a81efd4" class="main-col-div">
        <h3 class="heading-3">attention level</h3>
        <p class="paragraph-content">Growing numbers of users have begun to think of TV as a secondary, or background activity. Part of what makes it so appealing is that it requires no active input once you get started watching. You can make meals, chat with friends, workout, or browse the web while the show plays in the background. People usually choose shows that feel low-stakes, breezy comedies or rewatches, so that they don’t feel they have to miss anything, stop, and rewind.<br><br>With novel technologies in the near future, we could alleviate these pain points by monitoring attention levels and using it to feed the level of attention the content requires. We considered various modality changes to facilitate this process and maintain user engagement in content, including:</p><img src="images/Frame-3.svg" loading="lazy" alt="">
        <p class="paragraph-content">Testing of these three different engagement retainment techniques showed that users <span class="bold-text"><strong>preferred</strong></span> <span class="text-span-6"><strong>pausing</strong></span> if they were partaking in a high-cognitive task and audio descriptions if they were partaking in a low-cognitive task. The rewinding method was the least popular amongst users due to it’s potential to disrupt the viewing experience, rather than aid it.</p>
        <h3 class="heading-1">Modality Switch Method Preferred by Users</h3><img src="images/50-1.svg" loading="lazy" alt="" class="image-28">
        <p class="paragraph-content">However, while we typically envision this being used with rewatches or low-effort content, due to the nuances of content-viewing and the circumstantial aspects of each unique viewer’s ever-changing context, some questions remain to consider in implementation of these engagement retainment methods.</p>
        <ul role="list" class="icongridbase2">
          <li id="w-node-f3b1017b-4e17-4e5b-1bd4-b5e9f876305e-78a22a58" class="questions">Should viewers get a choice of which modality to make a default?</li>
          <li class="questions">Should it be chosen for them?</li>
          <li class="questions">How does it differ on an initial watch compared to a rewatch?</li>
        </ul>
        <h3 class="heading-3">historical viewing data</h3>
      </div>
      <div id="historic-viewing" data-w-id="3d4b3f9c-d7b4-d835-f340-f2bc3b7daa3a" class="main-col-div">
        <p class="paragraph-content">In the framework of content adaptation, historical viewing preferences in combination with scene-level emotional responses allows for customization of visuals to more accurately target emotions on a by-person basis. Current recommendation systems may be able to ascertain that the user wants to watch horror content in order to be scared, but what is it that scares this particular user? Gore? Jump scares? Or maybe such things deter viewers from engaging in particular genres. Additive and subtractive measures could even be made to accentuate certain attributes while dampening others, increasing the enjoyment of already watched genres while potentially opening up new genres that were previously avoided due to age or personal restrictions.<br><br>The system’s understanding of scene-level content attributes and the user’s historical preferences not only helps narrow down the correct content to place in front of the viewer, but also brings about the potential to distribute individual content to a wider market.</p><img src="images/People-watching-the-news-amico.svg" loading="lazy" data-w-id="37e7f955-1c1c-5bc0-5656-e00295c66938" alt="" class="image-3 customimage1">
        <h3 class="heading-3">user control</h3>
      </div>
      <div id="user-control" data-w-id="0a6614bf-4e8a-11a8-042c-df040610e201" class="main-col-div">
        <p class="paragraph-content"><br>While the goal for context-aware content is to have the narrative adapting automatically based on the user&#x27;s emotional responses, we learned from early prototyping that users have mixed emotions about the automated viewing experiences. It&#x27;s an experience they want to choose to have, not expect. <br><br>Integration into existing streaming platforms and the ability to toggle the feature on and off is imperative. AdaptiveWatch is a feature, a supplement, a way to enhance your viewing, but it&#x27;s not the rule.<br></p><img src="images/Frame-4streamingservices.png" loading="lazy" sizes="(max-width: 479px) 74vw, (max-width: 767px) 75vw, (max-width: 991px) 60vw, 65vw" srcset="images/Frame-4streamingservices-p-500.png 500w, images/Frame-4streamingservices.png 789w" alt="Streaming Services: Netflix, Hulu, Disney Plus, and Amazon Prime Video" class="image-25">
        <h2 class="heading-1-copy">What it Looks Like</h2><img src="images/ezgif.com-gif-maker-5.gif" loading="lazy" alt="A cursor moves around a page selecting adaptive watch options on Hulu" class="image-27">
        <h3 class="heading-3">risk factors</h3>
      </div>
      <div id="risk-factors" data-w-id="70ac26a0-7082-a3fc-066e-4f2bb82421c0" class="main-col-div">
        <p class="paragraph-content">Most consumers own internet-connected devices that are meant to make their lives easier, like smartwatches (Apple Watch, Fitbit, etc.), smart speakers (Amazon Alexa, Google Home, etc.), and security cameras (Ring, Nest, etc.) These devices are not necessarily always recording or monitoring us, but they are always listening - ready for a command to be given that would help them serve us better. <br><br>While the novel solution proposed by our team can blend seamlessly into user’s lives by utilizing these common smart home and wearable technologies, there is added risk to be considered if it were democratized as a product.<br></p><img src="images/PrototypeImg_4.png" loading="lazy" sizes="(max-width: 479px) 74vw, (max-width: 767px) 60vw, (max-width: 991px) 48vw, 52vw" srcset="images/PrototypeImg_4-p-500.png 500w, images/PrototypeImg_4.png 766w" alt="A graph showing various risks of Adaptive Watch from low, to medium, to high." class="image-3">
        <p class="paragraph-content">At the <span class="text-span-8"><strong>lowest</strong></span> end of the risk spectrum is the potential for <span><strong class="bold-text-2">over-personalization</strong></span>. With something as broad as TV, we like variety and new experiences because we can experience them at our leisure and comfort. If a user has a preference for a certain type of content or genre, they could get trapped in a cyclical feedback loop as the system continues providing this content and cuts out everything else. <br><br>Next at <strong class="bold-text-3">medium</strong> risk is the <strong class="bold-text-4">autonomy</strong>, or lack thereof, that users have over the adaptations in their content. While we are working towards defining where the user-preferred line is between control and automation, this will continue to be a gray area as synthetic content requires some degree of yielding user control, no matter what. <br><br>Finally at the <strong class="bold-text-5">highest</strong> risk level is <strong class="bold-text-6">data privacy</strong> and the invasive nature of a context-aware system. While most users already own most of the devices required to listen for and input a user’s current state, they would need to be made aware of the way in which their devices would be registering their environmental data in order to function and this has the potential to worry consumers about their privacy, or lack thereof. A potential design solution to mitigate this could be to provide a system whose data lives remotely on an individual user’s device or server, rather than on the cloud, and cannot be shared outside of the system itself.<br></p>
      </div>
    </div>
  </div>
  <div class="page-background content">
    <div style="opacity:0" class="background-color"></div>
    <div class="_3d-scene"></div>
  </div>
  <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navbar w-nav">
    <div class="container-6 w-container">
      <nav role="navigation" class="nav-menu w-nav-menu">
        <div class="navlink_wrap">
          <a href="index.html" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2dc" class="navbar-link w-nav-link" data-index="0">Home</a>
          <div style="-webkit-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" class="navlink_underline"></div>
        </div>
        <div class="navlink_wrap">
          <a href="solution.html" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2e0" class="navbar-link w-nav-link" data-index="1">Solution</a>
          <div style="-webkit-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" class="navlink_underline"></div>
        </div>
        <div class="div-block-26">
          <div data-hover="1" data-delay="0" class="dropdown w-dropdown">
            <div class="dropdown-toggle navlink_wrap w-dropdown-toggle">
              <div class="dropdown-text">process</div>
            </div>
            <nav class="dropdown-list w-dropdown-list">
              <a href="process.html" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2ed" style="color:rgb(103,131,230)" class="dropdown-link w-dropdown-link navbar-link" data-index="2">Research</a>
              <a href="process-prototyping.html" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2ef" style="color:rgb(103,131,230)" aria-current="page" class="dropdown-link w-dropdown-link w--current navbar-link" data-index="3">Prototyping</a>
            </nav>
          </div>
        </div>
        <div class="navlink_wrap">
          <a href="team.html" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2e4" class="navbar-link w-nav-link" data-index="4">team</a>
          <div style="-webkit-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" class="navlink_underline"></div>
        </div>
        <div class="navlink_wrap">
          <a href="https://interdigitalatcmu.medium.com/" data-w-id="5837be2a-e657-eb2a-edd3-68cd79cdf2f2" target="_blank" class="navbar-link w-nav-link" data-index="3">blog</a>
          <div style="-webkit-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-moz-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);-ms-transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0);transform:translate3d(-100%, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0) skew(0, 0)" class="navlink_underline"></div>
        </div>
      </nav>
      <div class="menu-button w-nav-button">
        <div class="icon w-icon-nav-menu"></div>
      </div>
    </div>
  </div>
  <div class="home-section-spacer bottom-nav wf-section">
    <a href="team.html" id="w-node-_4b4915e5-6ddb-fe28-815a-0340cad09c39-78a22a58" class="link-2"><span class="text-span-15">Our team →</span></a>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=607dcf321ef9c11104c38e66" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <script type="module">
    // Find the latest version by visiting https://unpkg.com/three.          
    import * as THREE from 'https://unpkg.com/three@0.127.0/build/three.module.js';
    import { GUI } from 'https://unpkg.com/three@0.127.0/examples/jsm/libs/dat.gui.module.js';
    import { GLTFLoader } from 'https://unpkg.com/three@0.127.0/examples/jsm/loaders/GLTFLoader.js';
    import { EffectComposer } from 'https://unpkg.com/three@0.127.0/examples/jsm/postprocessing/EffectComposer.js';
    import { RenderPass } from 'https://unpkg.com/three@0.127.0/examples/jsm/postprocessing/RenderPass.js';
    import { UnrealBloomPass } from 'https://unpkg.com/three@0.127.0/examples/jsm/postprocessing/UnrealBloomPass.js';
    import { SSAOPass } from 'https://unpkg.com/three@0.127.0/examples/jsm/postprocessing/SSAOPass.js';
    import Stats from 'https://unpkg.com/three@0.127.0/examples/jsm/libs/stats.module';

    const loader = new GLTFLoader();

    const scene = new THREE.Scene();
    loader.load('./Models/WebsiteScene.glb', function(gltf) {
        console.log(gltf.scene);
        scene.add(gltf.scene);
        configGLTBScene();
        init();
        animate();
        gsap.to(".content", {css: {backgroundColor: "rgba(0,0,0,0.4)"}, delay: 2})
    })

    // let stats = new Stats();

    const fov = 80;
    const aspect = 1.8;  // the canvas default
    const near = 0.1;
    const far = 1000;
    const camera = new THREE.PerspectiveCamera(fov, aspect, near, far);
    let cameraPosition = new THREE.Vector3(0, 2, 10);
    let cameraLookAtPosition = new THREE.Vector3(0, 0, 0); // TODO: Lerp between objects via tweening
    let lookAtObject = new THREE.Object3D();

    camera.position.set(cameraPosition.x, cameraPosition.y, cameraPosition.z);
    var renderer = new THREE.WebGLRenderer({
        antialias: true,
        alpha: true
    });

    var canvas = renderer.domElement;
    document.body.appendChild(canvas);

    var plane = new THREE.Plane(new THREE.Vector3(0, 0, 1), -10);
    var raycaster = new THREE.Raycaster();
    var mouse = new THREE.Vector2();
    var look = new THREE.Vector2();
    var pointOfIntersection = new THREE.Vector3();

    document.addEventListener("mousemove", onMouseMove, false);

    const PPParams = {
        exposure: 1,
        bloomStrength:  0.7,
        bloomThreshold: 0,
        bloomRadius: 0
    };

    function onMouseMove(event){
        mouse.x = ( event.clientX / window.innerWidth ) * 2 - 1;
        mouse.y = - ( event.clientY / window.innerHeight ) * 2 + 1;
    }

    let videoTextIDs = {}
    function configGLTBScene() {
        // Set floor color
        scene.getObjectByName('Floor', true).material.color.setHex(0xF1F0E4);

        // PROTOTYPING PAGE
        scene.getObjectByName('ComputerScreen', true).material = new THREE.MeshBasicMaterial({ map:  new THREE.VideoTexture( document.getElementById('prototypeTV'))});
        document.getElementById('prototypeTV').play();
        scene.getObjectByName('ComputerScreen', true).material.side = THREE.DoubleSide;
    }

    let video, texture, deepFake, df_texture, material, mesh, box, composer, effectFXAA;
    function initDebugGUI() {
      const gui = new GUI();

      gui.add( PPParams, 'exposure', 0.1, 2 ).onChange( function ( value ) {

          renderer.toneMappingExposure = Math.pow( value, 4.0 );

      } );

      gui.add( PPParams, 'bloomThreshold', 0.0, 1.0 ).onChange( function ( value ) {

          bloomPass.threshold = Number( value );

      } );

      gui.add( PPParams, 'bloomStrength', 0.0, 3.0 ).onChange( function ( value ) {

          bloomPass.strength = Number( value );

      } );

      gui.add( PPParams, 'bloomRadius', 0.0, 1.0 ).step( 0.01 ).onChange( function ( value ) {

          bloomPass.radius = Number( value );

      } );

      gui.open();
    }


    function init() {
        lookAtObject.position.x = 0; lookAtObject.position.y = 0; lookAtObject.position.z = 0; 
        scene.add(lookAtObject)

        const color = 0x0;
        const density = 0.1;
        scene.fog = new THREE.FogExp2(color, density);

        // POST PROCESSING
        const renderScene = new RenderPass( scene, camera );
        const bloomPass = new UnrealBloomPass( new THREE.Vector2( window.innerWidth, window.innerHeight ), 1.5, 0.4, .85 );
        bloomPass.threshold = PPParams.bloomThreshold;
        bloomPass.strength = PPParams.bloomStrength;
        bloomPass.radius = PPParams.bloomRadius;
        renderer.setSize(window.innerWidth, window.innerHeight);

        const ssaoPass = new SSAOPass( scene, camera, window.innerWidth, window.innerHeight );
        ssaoPass.kernelRadius = 16;
        renderer.setClearColor( color, 1 );

        composer = new EffectComposer( renderer );
        composer.addPass( renderScene );
        composer.addPass( bloomPass );
        // composer.addPass(ssaoPass);

        // document.body.appendChild(stats.dom);
        // document.body.addEventListener('keydown', keyPressed);
        document.body.addEventListener('resize', onWindowResize)

        setupTween();
        SetCameraSpotlight($('body').data("index"));
        // initDebugGUI();
    }

    var easeAmount = 8;
    var sensitivity = 0.0005;
    function update() {
        if (spotLightHelper) spotLightHelper.update();
        lookAtObject.position.x = cameraLookAtPosition.x;
        lookAtObject.position.y = cameraLookAtPosition.y;
        lookAtObject.position.z = cameraLookAtPosition.z;

        look.x += (mouse.x-look.x)/easeAmount;
        look.y += (mouse.y-look.y)/easeAmount;
        
        camera.position.x = cameraPosition.x + look.x * window.innerWidth * sensitivity;
        camera.position.y = cameraPosition.y + look.y * window.innerHeight * sensitivity;
        camera.position.z = cameraPosition.z;

        camera.lookAt(lookAtObject.position);
        // stats.update();
        TWEEN.update();
    }

    function animate() 
    {
        requestAnimationFrame( animate );
        composer.render();
        // render();		
        update();
    }

    function render() 
    {	
        if ( video.readyState === video.HAVE_ENOUGH_DATA ) 
        {
            if ( texture ) 
                texture.needsUpdate = true;
        }

        if (resize(renderer)) {
            camera.aspect = canvas.clientWidth / canvas.clientHeight;
            camera.updateProjectionMatrix();
        }

        renderer.render( scene, camera );
    }

    function resize(renderer) {
        const canvas = renderer.domElement;
        const width = canvas.clientWidth;
        const height = canvas.clientHeight;
        const needResize = canvas.width !== width || canvas.height !== height;
        if (needResize) {
            renderer.setSize(width, height, false);
        }
        return needResize;
    }

    function onWindowResize() {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize( window.innerWidth, window.innerHeight );
    }

    function SetCameraSpotlight(newTweenIndex) {
        cameraTweenIndex = newTweenIndex;
        updateCameraTransform();
        updateSpotlight();
    }

    let cameraSpotlightConfig = [
        {
            focusObj: "FutureTV",
            positionName: "Position_Main",
            lookAt: null,
            cameraPos: null,
            lightColor: 0xF1F0E4,
            lightIntensity: 2,
        },
        {
            focusObj: "CinemaScreen",
            positionName: "Position_Solution",
            lookAt: null,
            cameraPos: null,
            lightColor: 0xF1F0E4,
            lightIntensity: 2,
        },
        {
            focusObj: "SonyTV",
            positionName: "Position_Research",
            lookAt: null,
            cameraPos: null,
            lightColor: 0xF1F0E4,
            lightIntensity: 2,
        },
        {
            focusObj: "ComputerScreen",
            positionName: "Position_Prototype",
            lookAt: null,
            cameraPos: null,
            lightColor: 0xF1F0E4,
            lightIntensity: 1.5,
        },
        {
            focusObj: "TeamPanel",
            positionName: "Position_About",
            lookAt: null,
            cameraPos: null,
            lightColor: 0xF1F0E4,
            lightIntensity: 1.5,
        }
    ]

    let default_color = 0xF1F0E4;
    let cameraTweenIndex = 0;
    let spotlight, spotLightHelper;
    function setupTween() {
        TWEEN.removeAll();

        spotlight = createSpotlight(default_color);
        scene.add(spotlight);

        // spotLightHelper = new THREE.SpotLightHelper( spotlight );
        // scene.add( spotLightHelper );

        cameraSpotlightConfig.forEach((config, i) => {
            // console.log(config);
            let obj = scene.getObjectByName(config.focusObj, true);
            const camPos = scene.getObjectByName(config.positionName, true).position;
            config.lookAt = new THREE.Vector3(obj.position.x, obj.position.y, obj.position.z);
            config.cameraPos = new THREE.Vector3(camPos.x, camPos.y, camPos.z);
        })
    }

    function updateCameraTransform() {
        const temp = cameraSpotlightConfig[cameraTweenIndex];

        let tempPos = cameraPosition;
        let newCamPos = temp.cameraPos;
        let tempLookAt = lookAtObject.position;
        let newLookAt = temp.lookAt;

        // console.log(newCamPos);

        new TWEEN.Tween(tempPos)
        .to(newCamPos, 2000)
        .easing (TWEEN.Easing.Cubic.InOut)
        .onUpdate(function() { cameraPosition = tempPos; })
        .start();

        new TWEEN.Tween(tempLookAt)
        .to(newLookAt, 2000)
        .easing (TWEEN.Easing.Cubic.InOut)
        .onUpdate(function() { cameraLookAtPosition = tempLookAt; })
        .start();
    }

    function createSpotlight(color) {
        const newObj = new THREE.SpotLight(color, 2);

        newObj.castShadow = true;
        newObj.angle = 0.5;
        newObj.penumbra = 0.5;
        newObj.decay = 2;
        newObj.distance = 20;
        newObj.target = lookAtObject;

        return newObj;
    }

    function updateSpotlight() {
        const temp = cameraSpotlightConfig[cameraTweenIndex];
        console.log(temp);

        let tempPosition = spotlight.target.position;
        let tempIntensity = spotlight.intensity;

        new TWEEN.Tween(spotlight.position)
        .to({x: temp.cameraPos.x, y: 10, z: temp.cameraPos.z}, 1000)
        .easing(TWEEN.Easing.Cubic.InOut)
        .start();

        spotlight.color.setHex(temp.lightColor);
        spotlight.intensity = temp.lightIntensity;
    }

    window.addEventListener( 'resize', function () {
			  camera.aspect = window.innerWidth / window.innerHeight;
			  camera.updateProjectionMatrix();
			  renderer.setSize( window.innerWidth, window.innerHeight );
			}, false );

    $('.navbar-link').on('click', function(e) {
        e.preventDefault();
        if ($(this).data("index") !== undefined && $(this).data("index") !== null) {
          SetCameraSpotlight($(this).data("index"));
          if (!this.href.includes("#") && !this.href.includes('interdigitalatcmu.medium.com')) {
            gsap.to(".content", {css: {backgroundColor: "rgba(0,0,0,1)"}, duration: 0.5})
            setTimeout(() => {
              window.location = this.href
            }, 1000);
          }
          else {
            window.open(this.href, '_blank');
          }
        }
    })

    $('.menuitem-link-2').on('click', function(e) {
      e.preventDefault();
      SetCameraSpotlight($(this).data("index"));
    })

    $('.home-menu-title').hover(function(e) {
      gsap.to($(this), {css: {color: '#6783E6'}, duration: 0.25});
    }, function(e) {
      if ($(this).hasClass('current-nav')) {
        gsap.to($(this), {css: {color: '#34F5C5'}, duration: 0.25});
      }
      else {
        gsap.to($(this), {css: {color: '#f1f0e4'}, duration: 0.25});
      }
    })

    $('#process_dropdown').hover(function(e) {
      $('#nav_dropdown').css('display', 'block');
    }, function(e) {
      $('#nav_dropdown').css('display', 'none');
    })

    $('.navbar-link').hover(function(e) {
      gsap.to($(this), {css: {color: '#34F5C5'}, duration: 0.25});
    }, function(e) {
      if ($(this).hasClass('current-nav')) {
        gsap.to($(this), {css: {color: '#34F5C5'}, duration: 0.25});
      }
      else {
        gsap.to($(this), {css: {color: '#f1f0e4'}, duration: 0.25});
      }
    })
  </script>
  <script type="text/javascript" src="js/gsap_anchor.js"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>